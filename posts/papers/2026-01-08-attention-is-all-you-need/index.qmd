---
title: "Attention Is All You Need"
date: 2026-01-08
categories: ["Transformer", "NLP"]
---

```{=html}
<div class="paper-meta">
  <div class="paper-info">
    <span class="paper-venue">ğŸ“ NeurIPS 2017</span>
    <span class="paper-authors">ğŸ‘¥ Vaswani, Shazeer, Parmar, et al.</span>
    <a href="https://arxiv.org/abs/1706.03762" target="_blank" class="paper-link">ğŸ“„ arXiv</a>
  </div>
  <div class="paper-tags">
    <span class="tag">transformer</span>
    <span class="tag">attention</span>
    <span class="tag">sequence-modeling</span>
  </div>
</div>
```

## ä¸€å¥è¯æ€»ç»“

ç”¨è‡ªæ³¨æ„åŠ›æ›¿ä»£ RNN/CNNï¼Œå®ç°æ›´é«˜æ•ˆçš„åºåˆ—å»ºæ¨¡ã€‚

## å…³é”®ç‚¹

-   ç»“æ„ï¼šEncoder-Decoderï¼Œå¤šå¤´æ³¨æ„åŠ› + å‰é¦ˆç½‘ç»œ
-   è®­ç»ƒæŠ€å·§ï¼šæ®‹å·®ã€LayerNormã€ä½ç½®ç¼–ç 

## æˆ‘å…³å¿ƒçš„é—®é¢˜ / å¤ç°è®¡åˆ’

-   åœ¨æˆ‘çš„æ•°æ®é›†ä¸Šä¸ RNN baseline çš„é€Ÿåº¦ä¸æ•ˆæœå¯¹æ¯”