---
title: "Normal Equations / MLE / LWLR"
description: "线性回归的正规方程、概率解释与局部加权线性回归"
date: 2026-01-16
categories: ["Note"]
tags: ["Linear Regression", "Normal Equation", "MLE", "LWLR"]
---

# 1.2 Normal Equations（正规方程）

## 1.2.1 问题回顾

线性回归的代价函数为： $$
J(\theta)
= \frac{1}{2}\sum_{i=1}^n
\left(h_\theta(x^{(i)}) - y^{(i)}\right)^2
$$

引入设计矩阵： $$
X =
\begin{bmatrix}
(x^{(1)})^T \\
\vdots \\
(x^{(n)})^T
\end{bmatrix},
\quad
y =
\begin{bmatrix}
y^{(1)} \\
\vdots \\
y^{(n)}
\end{bmatrix}
$$

则目标函数可写为： $$
J(\theta) = \frac{1}{2}\|X\theta - y\|_2^2
$$

## 1.2.2 一阶最优性条件

由于 $J(\theta)$ 是凸且可导函数，最优解 $\theta^*$ 满足： $$
\nabla_\theta J(\theta^*) = 0
$$

计算梯度： $$
\nabla_\theta J(\theta)
= X^T(X\theta - y)
$$

令其为零，得到正规方程： $$
\boxed{
X^T X \theta = X^T y
}
$$

## 1.2.3 闭式解

当 $X^T X$ 可逆时，正规方程的解为： $$
\boxed{
\theta^* = (X^T X)^{-1} X^T y
}
$$

该解为最小二乘问题的**解析解（closed-form solution）**。

## 1.2.4 与梯度下降的关系

-   正规方程：
    -   直接求解最优性条件
    -   不需要学习率
-   梯度下降：
    -   数值迭代方法
    -   在合适条件下收敛到同一 $\theta^*$

二者**最小化的是同一个目标函数，但计算路径不同**。

## 1.2.5 工程视角备注

-   当特征维度较大时，计算 $(X^T X)^{-1}$ 代价高且数值不稳定
-   现代机器学习实践中更常使用 GD / SGD 而非正规方程

# 1.3 Probabilistic Interpretation（概率解释）

## 1.3.1 噪声模型假设

假设数据生成过程为： $$
y^{(i)} = \theta^T x^{(i)} + \varepsilon^{(i)}
$$

其中噪声满足： $$
\varepsilon^{(i)} \sim \mathcal{N}(0, \sigma^2)
$$

因此： $$
p(y^{(i)} \mid x^{(i)}; \theta)
= \mathcal{N}(\theta^T x^{(i)}, \sigma^2)
$$

## 1.3.2 似然函数

假设样本独立同分布： $$
p(y \mid X; \theta)
= \prod_{i=1}^n p(y^{(i)} \mid x^{(i)}; \theta)
$$

对数似然函数为： $$
\ell(\theta)
= \sum_{i=1}^n
\log p(y^{(i)} \mid x^{(i)}; \theta)
$$

代入高斯分布形式： $$
\ell(\theta)
= -\frac{1}{2\sigma^2}
\sum_{i=1}^n
\left(y^{(i)} - \theta^T x^{(i)}\right)^2
+ \text{const}
$$

## 1.3.3 与最小二乘的等价性

最大化对数似然： $$
\max_\theta \ell(\theta)
\quad \Longleftrightarrow \quad
\min_\theta
\sum_{i=1}^n
\left(y^{(i)} - \theta^T x^{(i)}\right)^2
$$

因此得到结论：

> **最小二乘回归等价于高斯噪声假设下的最大似然估计（MLE）**

## 1.3.4 建模意义

-   损失函数并非随意选择
-   而是隐含了对数据噪声分布的假设
-   该思想将直接推广到：
    -   Logistic Regression
    -   Generalized Linear Models
    -   深度学习中的交叉熵损失

# 1.4 Locally Weighted Linear Regression（LWLR）

## 1.4.1 动机

全局线性模型： $$
h_\theta(x) = \theta^T x
$$

在非线性数据分布下可能拟合不足。

思想：在预测点附近，对样本赋予更大权重，进行**局部线性拟合**。

## 1.4.2 加权代价函数

对查询点 $x$，定义权重： $$
w^{(i)} =
\exp\left(
-\frac{\|x^{(i)} - x\|^2}{2\tau^2}
\right)
$$

其中$\tau$是带宽（bandwidth parameter），控制权重衰减速度。

> 这个权重函数，其实已经不是“随便一个权重”了，而是一个**核函数（kernel）**

加权最小二乘目标： $$
J(\theta)
= \frac{1}{2}
\sum_{i=1}^n
w^{(i)}
\left(h_\theta(x^{(i)}) - y^{(i)}\right)^2
$$

## 1.4.3 解析解形式

设权重矩阵： $$
W = \mathrm{diag}(w^{(1)}, \dots, w^{(n)})
$$

则局部参数解为： $$
\boxed{
\theta(x)
= (X^T W X)^{-1} X^T W y
}
$$

注意：$\theta$ 依赖于查询点 $x$ 该方法属于 **non-parametric learning**

## 1.4.4 理解层面总结

-   LWLR 是 kernel regression 的原型
-   展示了：
    -   全局模型 vs 局部模型
    -   参数化方法 vs 非参数方法